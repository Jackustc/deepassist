{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import types\n",
    "import uuid\n",
    "import time\n",
    "from copy import copy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from gym.envs.classic_control import rendering\n",
    "from pyglet.window import key as pygkey\n",
    "import gym\n",
    "from gym import spaces, wrappers\n",
    "\n",
    "import dill\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import zipfile\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "from baselines.deepq.models import mlp as deepq_mlp\n",
    "from baselines.deepq import learn as deepq_learn\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from baselines.deepq.simple import ActWrapper\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "import rospy\n",
    "from nav_msgs.msg import Odometry\n",
    "from sensor_msgs.msg import Image\n",
    "from geometry_msgs.msg import TransformStamped, Twist, Vector3\n",
    "from std_msgs.msg import Empty\n",
    "\n",
    "from transforms3d.euler import quat2euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'quadrotor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_vicon_state = None\n",
    "latest_ardrone_img = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vicon_callback(data):\n",
    "  global latest_vicon_state\n",
    "  latest_vicon_state = np.array([\n",
    "    data.transform.translation.x,\n",
    "    data.transform.translation.y,\n",
    "    data.transform.translation.z,\n",
    "    data.transform.rotation.x,\n",
    "    data.transform.rotation.y,\n",
    "    data.transform.rotation.z,\n",
    "    data.transform.rotation.w,\n",
    "  ])\n",
    "    \n",
    "def img_callback(data):\n",
    "  global latest_ardrone_img\n",
    "  latest_ardrone_img = np.fromstring(data.data, np.uint8).reshape((data.height, data.width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup [ROS](http://www.ros.org/) interfaces for [ARDrone](https://ardrone-autonomy.readthedocs.io/en/latest/) and [Vicon](https://github.com/ethz-asl/vicon_bridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscore\n",
    "cd ~/ardrone_ws; source devel/setup.bash; rosrun ardrone_autonomy ardrone_driver -ip 192.168.42.1\n",
    "cd ~/catkin_ws; source devel/setup.bash; roslaunch vicon_bridge vicon.launch\n",
    "rostopic echo ardrone/navdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospy.init_node('ed', anonymous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospy.Subscriber('vicon/ardrone/main', TransformStamped, vicon_callback)\n",
    "rospy.Subscriber('ardrone/image_raw', Image, img_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "publishers = {\n",
    "  'cmd_vel': rospy.Publisher('cmd_vel', Twist, queue_size=1),\n",
    "  'takeoff': rospy.Publisher('ardrone/takeoff', Empty, queue_size=1),\n",
    "  'land': rospy.Publisher('ardrone/land', Empty, queue_size=1),\n",
    "  'reset': rospy.Publisher('ardrone/reset', Empty, queue_size=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ardronecmdvel(data):\n",
    "  linear = Vector3(x=data['linear_x'], y=data['linear_y'], z=data['linear_z'])\n",
    "  angular = Vector3(x=data['angular_x'], y=data['angular_y'], z=data['angular_z'])\n",
    "  msg = Twist(linear=linear, angular=angular)\n",
    "  publishers['cmd_vel'].publish(msg)\n",
    "\n",
    "def ardronecmd(cmd):\n",
    "  publishers[cmd].publish(Empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twist_of_quat(quat):\n",
    "  return quat2euler(quat.elements)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_act_dim = 9\n",
    "n_obs_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOOP = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ARDrone(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "  \n",
    "  def __init__(self):      \n",
    "    self.action_space = spaces.Discrete(n_act_dim)\n",
    "    obs_low = -np.array([np.inf] * n_obs_dim)\n",
    "    obs_high = -obs_low\n",
    "    self.observation_space = spaces.Box(obs_low, obs_high)\n",
    "    \n",
    "    self.img_height = 368\n",
    "    self.img_width = 640\n",
    "    self.ground_contact_z_thresh = 0.1\n",
    "    self.max_pos = np.array([2, 2, 2])\n",
    "    self.max_ep_len = 100000\n",
    "    self.max_ep_duration = 30\n",
    "    self.start_z_thresh = 0.25\n",
    "    \n",
    "    self.max_xy_speed = 0.5\n",
    "    self.min_n_stab_noops = 25\n",
    "    self.n_stabilizing_noops = 0\n",
    "    self.landed = True\n",
    "    \n",
    "    self.terrain = self._init_terrain()\n",
    "    \n",
    "    self.goal_dist_thresh = 0.3\n",
    "    self.pad_pos = self._init_pad_pos()\n",
    "    self.obj_idx = None\n",
    "    self.obj_names = ['the red chair', 'the gray chair', 'the door', 'the white styrofoam boards that say R-TECH']\n",
    "    \n",
    "    self.translation = None\n",
    "    self.init_twist = None\n",
    "    self.prev_pos = None\n",
    "    self.prev_time = None\n",
    "    self.prev_shaping = None\n",
    "    self.curr_step = None\n",
    "    self.ep_start_time = None\n",
    "    self.prev_time_left = None\n",
    "        \n",
    "    self.viewer = None\n",
    "        \n",
    "    self.set_transform()\n",
    "    \n",
    "  def _init_terrain(self):\n",
    "    return np.ones((10, 10))\n",
    "    \n",
    "  def _get_vicon_pos(self):\n",
    "    return copy(latest_vicon_state)\n",
    "  \n",
    "  def _init_pad_pos(self):\n",
    "    return copy(latest_vicon_state)\n",
    "    \n",
    "  def set_transform(self, pos=None):\n",
    "    if pos is None:\n",
    "      pos = self._get_vicon_pos()\n",
    "    self.translation = -pos[:3]\n",
    "    self.init_twist = twist_of_quat(Quaternion(pos[3:7]))\n",
    "    \n",
    "  def _transform_pos(self, pos):\n",
    "    pos[:3] += self.translation\n",
    "    pos[6] = twist_of_quat(Quaternion(pos[3:7])) - self.init_twist\n",
    "    if pos[6] < 0:\n",
    "      pos[6] += 2*math.pi\n",
    "    return pos\n",
    "    \n",
    "  def _obs(self):\n",
    "    pos = self._transform_pos(self._get_vicon_pos())\n",
    "    pos = np.concatenate((pos[:3], pos[6:7]))\n",
    "    curr_time = time.time()\n",
    "    if self.prev_pos is not None:\n",
    "      vel = (pos - self.prev_pos) / (curr_time - self.prev_time)\n",
    "    else:\n",
    "      vel = np.zeros(pos.shape)\n",
    "    self.prev_pos = pos\n",
    "    self.prev_time = copy(curr_time)\n",
    "    \n",
    "    rot_ang = -pos[3]\n",
    "    rot = np.array([[np.cos(rot_ang), -np.sin(rot_ang)], [np.sin(rot_ang), np.cos(rot_ang)]])\n",
    "    delta_xy_to_goal = rot.dot(self._get_pad_pos()[:2] - pos[:2])\n",
    "        \n",
    "    return np.concatenate((pos, vel, delta_xy_to_goal))\n",
    "\n",
    "  def _get_pad_pos(self):\n",
    "    return self.pad_pos[:3] + self.translation\n",
    "  \n",
    "  def _at_site(self):\n",
    "    pos = self._transform_pos(self._get_vicon_pos())\n",
    "    return np.linalg.norm(pos[:2] - self._get_pad_pos()[:2]) <= self.goal_dist_thresh\n",
    "  \n",
    "  def _exec_action(self, action, xy_speed=0.1, z_speed=0.5, w_speed=1):\n",
    "    curr_xy_speed = np.linalg.norm(self._obs()[4:6])\n",
    "    if curr_xy_speed >= self.max_xy_speed:\n",
    "      self.n_stabilizing_noops = self.min_n_stab_noops\n",
    "    if self.n_stabilizing_noops > 0:\n",
    "      action = 8\n",
    "      self.n_stabilizing_noops -= 1\n",
    "    \n",
    "    vel = {\n",
    "      'linear_x': 0,\n",
    "      'linear_y': 0,\n",
    "      'linear_z': 0,\n",
    "      'angular_x': 0,\n",
    "      'angular_y': 0,\n",
    "      'angular_z': 0\n",
    "    }\n",
    "    \n",
    "    if action == 0:\n",
    "      vel['linear_x'] = xy_speed\n",
    "    elif action == 1:\n",
    "      vel['linear_x'] = -xy_speed\n",
    "    elif action == 2:\n",
    "      vel['linear_y'] = xy_speed\n",
    "    elif action == 3:\n",
    "      vel['linear_y'] = -xy_speed\n",
    "    elif action == 4:\n",
    "      pass\n",
    "    elif action == 5:\n",
    "      if curr_xy_speed < 0.1:\n",
    "        self._exec_cmd('land')\n",
    "        return\n",
    "    elif action == 6:\n",
    "      vel['angular_z'] = w_speed\n",
    "    elif action == 7:\n",
    "      vel['angular_z'] = -w_speed\n",
    "    elif action == NOOP: # 8\n",
    "      pass\n",
    "    else:\n",
    "      raise ValueError\n",
    "    \n",
    "    ardronecmdvel(vel)\n",
    "      \n",
    "  def _out_of_bounds(self, pos=None):\n",
    "    if pos is None:\n",
    "      pos = self._transform_pos(self._get_vicon_pos())\n",
    "    return (np.abs(pos[:3]) >= self.max_pos).any()\n",
    "  \n",
    "  def _step(self, action):\n",
    "    self._exec_action(action)\n",
    "    \n",
    "    obs = self._obs()\n",
    "        \n",
    "    dist_to_goal = np.linalg.norm(obs[-2:])\n",
    "    if dist_to_goal < 0.5*self.goal_dist_thresh:\n",
    "      shaping = -100*obs[2]\n",
    "    else:\n",
    "      shaping = -100*dist_to_goal\n",
    "    r = shaping\n",
    "    if self.prev_shaping is not None:\n",
    "      r -= self.prev_shaping\n",
    "    self.prev_shaping = shaping\n",
    "    \n",
    "    oob = self._out_of_bounds()\n",
    "    timeout = time.time() - self.ep_start_time > self.max_ep_duration\n",
    "    on_ground = obs[2] <= self.ground_contact_z_thresh or self.landed\n",
    "    done = oob or timeout or on_ground\n",
    "    \n",
    "    self.curr_step += 1\n",
    "    \n",
    "    at_loc = self._at_site()\n",
    "        \n",
    "    info = {}\n",
    "    if done:\n",
    "      self._land()\n",
    "      info['duration'] = time.time() - self.ep_start_time\n",
    "      info['oob'] = oob\n",
    "      info['timeout'] = timeout\n",
    "      info['on_ground'] = on_ground\n",
    "      info['at_loc'] = at_loc\n",
    "        \n",
    "      if oob:\n",
    "        print('You flew out of bounds.')\n",
    "      if timeout:\n",
    "        print('You ran out of time.')\n",
    "      if at_loc and on_ground:\n",
    "        print('You landed on the landing pad.')\n",
    "      elif on_ground and not at_loc:\n",
    "        print('You missed the landing pad.')\n",
    "      \n",
    "      at_rot = None\n",
    "      while at_rot is None or at_rot not in ['y', 'n']:\n",
    "        at_rot = input('Is the camera pointed at %s? (y/n): ' % self.obj_names[self.obj_idx])\n",
    "      print('')\n",
    "      info['at_rot'] = at_rot\n",
    "      info['final_img'] = copy(latest_ardrone_img)\n",
    "      info['goal_obj'] = self.obj_names[self.obj_idx]\n",
    "      \n",
    "      if oob or (on_ground and not at_loc):\n",
    "        r = -100\n",
    "      elif at_loc and on_ground and at_rot == 'y':\n",
    "        r = 100\n",
    "    \n",
    "    return obs, r, done, info\n",
    "    \n",
    "  def _takeoff(self):\n",
    "    self._exec_cmd('takeoff')\n",
    "    pos = self._transform_pos(self._get_vicon_pos())\n",
    "    while pos[2] < self.start_z_thresh:\n",
    "      pos = self._transform_pos(self._get_vicon_pos())\n",
    "    self.landed = False\n",
    "      \n",
    "  def _exec_cmd(self, cmd):\n",
    "    if cmd == 'land':\n",
    "      os.system('rostopic pub --once ardrone/land std_msgs/Empty')\n",
    "      self.landed = True\n",
    "    else:\n",
    "      ardronecmd(cmd)\n",
    "    \n",
    "  def _land(self):\n",
    "    self._exec_cmd('land')\n",
    "    \n",
    "  def _reset(self):\n",
    "    self._land()\n",
    "    \n",
    "    self.prev_pos = None\n",
    "    self.prev_time = None\n",
    "    self.prev_shaping = None\n",
    "    self.prev_obs = None\n",
    "    self.curr_step = 0\n",
    "    self.n_stabilizing_noops = 0\n",
    "    self.prev_time_left = None\n",
    "    \n",
    "    self.obj_idx = random.randint(0, 3)\n",
    "    \n",
    "    print('Your goal for this episode is to point the camera at %s.' % self.obj_names[self.obj_idx])\n",
    "    \n",
    "    self.pad_pos = -self.translation\n",
    "        \n",
    "    pause = input('Experimenter, place the drone and hit ENTER: ')\n",
    "    while self._out_of_bounds():\n",
    "      pause = input('The drone is out of bounds. Experimenter, try again and hit ENTER: ')\n",
    "    \n",
    "    self._takeoff()\n",
    "        \n",
    "    print('The episode has begun.')\n",
    "      \n",
    "    self.ep_start_time = time.time()\n",
    "    return self._step(NOOP)[0] # noop\n",
    "  \n",
    "  def _render(self, mode='human', close=False, godmode=False):\n",
    "    if close:\n",
    "      if self.viewer is not None:\n",
    "        self.viewer.close()\n",
    "        self.viewer = None\n",
    "      return\n",
    "    \n",
    "    if self.viewer is None:\n",
    "      self.viewer = rendering.SimpleImageViewer()\n",
    "    \n",
    "    if godmode:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = plt.axes(xlim=(-0.5, self.terrain.shape[0] - 0.5), ylim=(-0.5, self.terrain.shape[1] - 0.5))\n",
    "        pursuer_pos = self._transform_pos(self._get_vicon_pos())\n",
    "        evader_loc = (self._get_pad_pos()[:2] + self.max_pos[:2]) / (2 * self.max_pos[:2])\n",
    "        evader_loc[1] = 1 - evader_loc[1]\n",
    "        evader_loc[0] = evader_loc[0] * self.terrain.shape[0]-0.5\n",
    "        evader_loc[1] = evader_loc[1] * self.terrain.shape[1]-0.5\n",
    "        ax.scatter([evader_loc[1]], [evader_loc[0]], s=500, color='red', linewidth=0, alpha=0.75)\n",
    "        pursuer_size = 500 + 10000 * pursuer_pos[2] / self.max_pos[2]\n",
    "        pursuer_loc = (pursuer_pos[:2] + self.max_pos[:2]) / (2 * self.max_pos[:2])\n",
    "        pursuer_loc[1] = 1 - pursuer_loc[1]\n",
    "        pursuer_loc[0] = pursuer_loc[0] * self.terrain.shape[0]-0.5\n",
    "        pursuer_loc[1] = pursuer_loc[1] * self.terrain.shape[1]-0.5\n",
    "        ax.scatter([pursuer_loc[1]], [pursuer_loc[0]], s=pursuer_size, c='blue', linewidth=0, alpha=0.75)\n",
    "        ang = self._obs()[3]\n",
    "        ax.set_ylabel('ang = %f' % ang, fontsize=30)\n",
    "\n",
    "        ax.set_title('%d seconds left' % (self.max_ep_duration - ((time.time() - self.ep_start_time) if self.ep_start_time is not None else 0)),\n",
    "                     fontsize=30)\n",
    "        ax.set_xlabel('z = %0.2f' % (pursuer_pos[2] / self.max_pos[2]),\n",
    "                     fontsize=30)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        agg = canvas.switch_backends(FigureCanvas)\n",
    "        agg.draw()\n",
    "        width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "        self.viewer.imshow(np.fromstring(agg.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3))\n",
    "        plt.close()\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = plt.axes()\n",
    "        time_left = self.max_ep_duration - ((time.time() - self.ep_start_time) if self.ep_start_time is not None else 0)\n",
    "        time_left = max(0, time_left - 5)\n",
    "        ax.set_xlabel('%d seconds left' % (time_left),\n",
    "                     fontsize=60)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        agg = canvas.switch_backends(FigureCanvas)\n",
    "        agg.draw()\n",
    "        width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "        img_arr = np.fromstring(agg.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)\n",
    "        self.viewer.imshow(img_arr)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = ARDrone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ep_len = env.max_ep_len\n",
    "def run_ep(policy, env, max_ep_len=max_ep_len, render=False, pilot_is_human=False):\n",
    "    if pilot_is_human:\n",
    "      global human_agent_action\n",
    "      human_agent_action = init_human_action()\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalr = 0.\n",
    "    trajectory = [obs]\n",
    "    actions = []\n",
    "    for step_idx in range(max_ep_len+1):\n",
    "        if done:\n",
    "            break\n",
    "        action = policy(obs[None, :])\n",
    "        obs, r, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        trajectory.append(obs)\n",
    "        if render:\n",
    "          env.render()\n",
    "        totalr += r\n",
    "    outcome = r if r % 100 == 0 else 0\n",
    "    return totalr, outcome, trajectory, actions, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noop_pilot_policy(obs):\n",
    "  return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_human_action = lambda: NOOP\n",
    "human_agent_action = init_human_action()\n",
    "\n",
    "action_of_key = {\n",
    "  pygkey.UP: 0,\n",
    "  pygkey.DOWN: 1,\n",
    "  pygkey.LEFT: 2,\n",
    "  pygkey.RIGHT: 3,\n",
    "  pygkey.W: 4,\n",
    "  pygkey.S: 5,\n",
    "  pygkey.A: 6,\n",
    "  pygkey.D: 7,\n",
    "}\n",
    "\n",
    "def key_press(key, mod):\n",
    "  k = int(key)\n",
    "  if k in action_of_key:\n",
    "    global human_agent_action\n",
    "    human_agent_action = action_of_key[k]\n",
    "\n",
    "def key_release(key, mod):\n",
    "  k = int(key)\n",
    "  if k in action_of_key:\n",
    "    global human_agent_action\n",
    "    human_agent_action = 8\n",
    "      \n",
    "def human_pilot_policy(obs):\n",
    "  return human_agent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tf_vars(scope, path):\n",
    "  sess = U.get_session()\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.save(sess, save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tf_vars(scope, path):\n",
    "  sess = U.get_session()\n",
    "  saver = tf.train.Saver([v for v in tf.global_variables() if v.name.startswith(scope + '/')])\n",
    "  saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train assistive copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_training_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_q_func = lambda: deepq_mlp([64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copilot_dqn_learn_kwargs = {\n",
    "  'lr': 1e-4,\n",
    "  'exploration_fraction': 0.1,\n",
    "  'exploration_final_eps': 0.02,\n",
    "  'target_network_update_freq': 3000,\n",
    "  'print_freq': 100,\n",
    "  'num_cpu': 5,\n",
    "  'gamma': 0.99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_encode(i, n=n_act_dim):\n",
    "    x = np.zeros(n)\n",
    "    x[i] = 1\n",
    "    return x\n",
    "\n",
    "def onehot_decode(x):\n",
    "    l = np.nonzero(x)[0]\n",
    "    assert len(l) == 1\n",
    "    return l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_co_env(pilot_policy, **extras):\n",
    "  env = ARDrone()\n",
    "  env.unwrapped.pilot_policy = pilot_policy\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_build_act(make_obs_ph, q_func, num_actions, scope=\"deepq\", reuse=None, using_control_sharing=True):\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    observations_ph = U.ensure_tf_input(make_obs_ph(\"observation\"))\n",
    "    if using_control_sharing:\n",
    "      pilot_action_ph = tf.placeholder(tf.int32, (), name='pilot_action')\n",
    "      pilot_tol_ph = tf.placeholder(tf.float32, (), name='pilot_tol')\n",
    "    else:\n",
    "      eps = tf.get_variable(\"eps\", (), initializer=tf.constant_initializer(0))\n",
    "      stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n",
    "      update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n",
    "\n",
    "    q_values = q_func(observations_ph.get(), num_actions, scope=\"q_func\")\n",
    "\n",
    "    batch_size = tf.shape(q_values)[0]\n",
    "\n",
    "    if using_control_sharing:\n",
    "      q_values -= tf.reduce_min(q_values, axis=1)\n",
    "      opt_actions = tf.argmax(q_values, axis=1, output_type=tf.int32)\n",
    "      opt_q_values = tf.reduce_max(q_values, axis=1)\n",
    "\n",
    "      batch_idxes = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
    "      reshaped_batch_size = tf.reshape(batch_size, [1])\n",
    "\n",
    "      pi_actions = tf.tile(tf.reshape(pilot_action_ph, [1]), reshaped_batch_size)\n",
    "      pi_act_idxes = tf.concat([batch_idxes, tf.reshape(pi_actions, [batch_size, 1])], axis=1)\n",
    "      pi_act_q_values = tf.gather_nd(q_values, pi_act_idxes)\n",
    "      \n",
    "      actions = tf.where(pi_act_q_values >= (1 - pilot_tol_ph) * opt_q_values, pi_actions, opt_actions)\n",
    "      \n",
    "      act = U.function(inputs=[\n",
    "        observations_ph, pilot_action_ph, pilot_tol_ph\n",
    "      ],\n",
    "                       outputs=[actions])\n",
    "    else:\n",
    "      deterministic_actions = tf.argmax(q_values, axis=1)\n",
    "\n",
    "      random_actions = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=num_actions, dtype=tf.int64)\n",
    "      chose_random = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < eps\n",
    "      stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)\n",
    "\n",
    "      output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)\n",
    "      update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))\n",
    "      act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph],\n",
    "                       outputs=[output_actions],\n",
    "                       givens={update_eps_ph: -1.0, stochastic_ph: True},\n",
    "                       updates=[update_eps_expr])\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_build_train(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping=None, gamma=1.0,\n",
    "    double_q=True, scope=\"deepq\", reuse=None, using_control_sharing=True):\n",
    "    act_f = co_build_act(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse, using_control_sharing=using_control_sharing)\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # set up placeholders\n",
    "        obs_t_input = U.ensure_tf_input(make_obs_ph(\"obs_t\"))\n",
    "        act_t_ph = tf.placeholder(tf.int32, [None], name=\"action\")\n",
    "        rew_t_ph = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "        obs_tp1_input = U.ensure_tf_input(make_obs_ph(\"obs_tp1\"))\n",
    "        done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n",
    "        importance_weights_ph = tf.placeholder(tf.float32, [None], name=\"weight\")\n",
    "\n",
    "        obs_t_input_get = obs_t_input.get()\n",
    "        obs_tp1_input_get = obs_tp1_input.get()\n",
    "\n",
    "        # q network evaluation\n",
    "        q_t = q_func(obs_t_input_get, num_actions, scope='q_func', reuse=True)  # reuse parameters from act\n",
    "        q_func_vars = U.scope_vars(U.absolute_scope_name('q_func'))\n",
    "\n",
    "        # target q network evalution\n",
    "        q_tp1 = q_func(obs_tp1_input_get, num_actions, scope=\"target_q_func\")\n",
    "        target_q_func_vars = U.scope_vars(U.absolute_scope_name(\"target_q_func\"))\n",
    "\n",
    "        # q scores for actions which we know were selected in the given state.\n",
    "        q_t_selected = tf.reduce_sum(q_t * tf.one_hot(act_t_ph, num_actions), 1)\n",
    "\n",
    "        # compute estimate of best possible value starting from state at t + 1\n",
    "        if double_q:\n",
    "            q_tp1_using_online_net = q_func(obs_tp1_input_get, num_actions, scope='q_func', reuse=True)\n",
    "            q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1)\n",
    "            q_tp1_best = tf.reduce_sum(q_tp1 * tf.one_hot(q_tp1_best_using_online_net, num_actions), 1)\n",
    "        else:\n",
    "            q_tp1_best = tf.reduce_max(q_tp1, 1)\n",
    "        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n",
    "\n",
    "        # compute RHS of bellman equation\n",
    "        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "\n",
    "        # compute the error (potentially clipped)\n",
    "        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n",
    "        errors = U.huber_loss(td_error)\n",
    "        weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n",
    "\n",
    "        # compute optimization op (potentially with gradient clipping)\n",
    "        if grad_norm_clipping is not None:\n",
    "            optimize_expr = U.minimize_and_clip(optimizer,\n",
    "                                                weighted_error,\n",
    "                                                var_list=q_func_vars,\n",
    "                                                clip_val=grad_norm_clipping)\n",
    "        else:\n",
    "            optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)\n",
    "\n",
    "        # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "        update_target_expr = []\n",
    "        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_expr.append(var_target.assign(var))\n",
    "        update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "        # Create callable functions\n",
    "        train = U.function(\n",
    "            inputs=[\n",
    "                obs_t_input,\n",
    "                act_t_ph,\n",
    "                rew_t_ph,\n",
    "                obs_tp1_input,\n",
    "                done_mask_ph,\n",
    "                importance_weights_ph\n",
    "            ],\n",
    "            outputs=td_error,\n",
    "            updates=[optimize_expr]\n",
    "        )\n",
    "        update_target = U.function([], [], updates=[update_target_expr])\n",
    "\n",
    "        q_values = U.function([obs_t_input], q_t)\n",
    "\n",
    "    return act_f, train, update_target, {'q_values': q_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_dqn_learn(\n",
    "    env,\n",
    "    q_func,\n",
    "    lr=1e-3,\n",
    "    max_timesteps=100000,\n",
    "    buffer_size=50000,\n",
    "    train_freq=1,\n",
    "    batch_size=32,\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=10000,\n",
    "    learning_starts=1000,\n",
    "    gamma=1.0,\n",
    "    target_network_update_freq=500,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    num_cpu=5,\n",
    "    callback=None,\n",
    "    scope='deepq',\n",
    "    pilot_tol=0,\n",
    "    pilot_is_human=False,\n",
    "    reuse=False):\n",
    "    \n",
    "    # Create all the functions necessary to train the model\n",
    "\n",
    "    sess = U.get_session()\n",
    "    if sess is None:\n",
    "      sess = U.make_session(num_cpu=num_cpu)\n",
    "      sess.__enter__()\n",
    "\n",
    "    def make_obs_ph(name):\n",
    "        return U.BatchInput(env.observation_space.shape, name=name)\n",
    "      \n",
    "    using_control_sharing = pilot_tol > 0\n",
    "    \n",
    "    act, train, update_target, debug = co_build_train(\n",
    "        scope=scope,\n",
    "        make_obs_ph=make_obs_ph,\n",
    "        q_func=q_func,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "        gamma=gamma,\n",
    "        grad_norm_clipping=10,\n",
    "        reuse=reuse,\n",
    "        using_control_sharing=using_control_sharing\n",
    "    )\n",
    "    \n",
    "    act_params = {\n",
    "        'make_obs_ph': make_obs_ph,\n",
    "        'q_func': q_func,\n",
    "        'num_actions': env.action_space.n,\n",
    "    }\n",
    "\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "    \n",
    "    if max_timesteps == 0:\n",
    "      return ActWrapper(act, act_params)\n",
    "\n",
    "    episode_trajectories = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_outcomes = []\n",
    "    saved_mean_reward = None\n",
    "    obs = env.reset()\n",
    "    prev_t = 0\n",
    "    episode_reward = 0\n",
    "    rollouts = []\n",
    "    episode_trajectory = []\n",
    "    episode_action = []\n",
    "    \n",
    "    if pilot_is_human:\n",
    "      global human_agent_action\n",
    "      human_agent_action = init_human_action()\n",
    "    \n",
    "    #if not using_control_sharing:\n",
    "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * max_timesteps),\n",
    "                               initial_p=1.0,\n",
    "                               final_p=exploration_final_eps)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        model_saved = False\n",
    "        model_file = os.path.join(td, 'model')\n",
    "        for t in range(max_timesteps):\n",
    "            episode_trajectory.append(obs)\n",
    "            \n",
    "            act_kwargs = {}\n",
    "            if using_control_sharing:\n",
    "              pilot_action = env.unwrapped.pilot_policy(obs[None, :n_obs_dim])\n",
    "              act_kwargs['pilot_action'] = pilot_action\n",
    "              act_kwargs['pilot_tol'] = pilot_tol if pilot_action != 8 else 0\n",
    "            else:\n",
    "              act_kwargs['update_eps'] = exploration.value(t)\n",
    "              \n",
    "            action = act(obs[None, :], **act_kwargs)[0][0]\n",
    "            #if np.random.random() < exploration.value(t):\n",
    "            #  action = random.randint(0, 8) # DEBUG\n",
    "            new_obs, rew, done, info = env.step(action)\n",
    "            episode_action.append(action)\n",
    "\n",
    "            if pilot_is_human:\n",
    "              env.render()\n",
    "\n",
    "            # Store transition in the replay buffer.\n",
    "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "            obs = new_obs\n",
    "\n",
    "            episode_reward += rew\n",
    "\n",
    "            if done:\n",
    "                if t > learning_starts:\n",
    "                  for _ in range(t - prev_t):\n",
    "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
    "                    weights, batch_idxes = np.ones_like(rewards), None\n",
    "                    td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "\n",
    "                obs = env.reset()\n",
    "\n",
    "                episode_outcomes.append(rew)\n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_trajectories.append(episode_trajectory + [new_obs])\n",
    "                episode_actions.append(episode_action)\n",
    "                episode_trajectory = []\n",
    "                episode_action = []\n",
    "                episode_reward = 0\n",
    "\n",
    "                if pilot_is_human:\n",
    "                  global human_agent_action\n",
    "                  human_agent_action = init_human_action()\n",
    "\n",
    "                prev_t = t\n",
    "                    \n",
    "                if pilot_is_human:\n",
    "                  time.sleep(2)\n",
    "\n",
    "            if t > learning_starts and t % target_network_update_freq == 0:\n",
    "                # Update target network periodically.\n",
    "                update_target()\n",
    "\n",
    "            mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
    "            mean_100ep_succ = round(np.mean([1 if x==100 else 0 for x in episode_outcomes[-100:]]), 2)\n",
    "            mean_100ep_crash = round(np.mean([1 if x==-100 else 0 for x in episode_outcomes[-100:]]), 2)\n",
    "            num_episodes = len(episode_rewards)\n",
    "            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
    "                logger.record_tabular(\"steps\", t)\n",
    "                logger.record_tabular(\"episodes\", num_episodes)\n",
    "                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "                logger.record_tabular(\"mean 100 episode succ\", mean_100ep_succ)\n",
    "                logger.record_tabular(\"mean 100 episode crash\", mean_100ep_crash)\n",
    "                logger.dump_tabular()\n",
    "\n",
    "            if checkpoint_freq is not None and t > learning_starts and num_episodes > 100 and t % checkpoint_freq == 0 and (saved_mean_reward is None or mean_100ep_reward > saved_mean_reward):\n",
    "                if print_freq is not None:\n",
    "                    print('Saving model due to mean reward increase:')\n",
    "                    print(saved_mean_reward, mean_100ep_reward)\n",
    "                U.save_state(model_file)\n",
    "                model_saved = True\n",
    "                saved_mean_reward = mean_100ep_reward\n",
    "\n",
    "        if model_saved:\n",
    "            U.load_state(model_file)\n",
    "\n",
    "    reward_data = {\n",
    "      'rewards': episode_rewards,\n",
    "      'outcomes': episode_outcomes,\n",
    "      'trajectories': episode_trajectories,\n",
    "      'actions': episode_actions\n",
    "    }\n",
    "          \n",
    "    return ActWrapper(act, act_params), reward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_co_policy(\n",
    "  env, scope=None, pilot_tol=0, pilot_is_human=False, \n",
    "  n_eps=n_training_episodes, copilot_scope=None, \n",
    "  copilot_q_func=None,\n",
    "  reuse=False, **extras):\n",
    "  \n",
    "  if copilot_scope is not None:\n",
    "    scope = copilot_scope\n",
    "  elif scope is None:\n",
    "    scope = str(uuid.uuid4())\n",
    "  q_func = copilot_q_func if copilot_scope is not None else make_q_func()\n",
    "    \n",
    "  return (scope, q_func), co_dqn_learn(\n",
    "    env,\n",
    "    scope=scope,\n",
    "    q_func=q_func,\n",
    "    max_timesteps=max_ep_len*n_eps,\n",
    "    pilot_tol=pilot_tol,\n",
    "    pilot_is_human=pilot_is_human,\n",
    "    reuse=reuse,\n",
    "    **copilot_dqn_learn_kwargs\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pretrained copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copilot_path = os.path.join(data_dir, 'pretrained_noop_copilot')\n",
    "copilot_scope = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_env = make_co_env(noop_pilot_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(scope, q_func), raw_copilot_policy = make_co_policy(\n",
    "  co_env, pilot_tol=1e-3, pilot_is_human=False, n_eps=0,\n",
    "  copilot_scope=copilot_scope,\n",
    "  copilot_q_func=make_q_func(),\n",
    "  reuse=False,\n",
    "  pilot_policy=noop_pilot_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tf_vars(copilot_scope, copilot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate solo pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pilot_id = 'spike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_eval_eps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.unwrapped.viewer.window.on_key_press = key_press\n",
    "env.unwrapped.viewer.window.on_key_release = key_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollouts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollouts_checkpoint_path = os.path.join(data_dir, '%s_rollouts_checkpoint.pkl' % pilot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(rollouts_checkpoint_path, 'rb') as f:\n",
    "  rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(n_eval_eps - len(rollouts)):\n",
    "  print('This will be episode %d of %d' % (len(rollouts)+1, n_eval_eps))\n",
    "  rollouts.append(run_ep(human_pilot_policy, env, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(rollouts_checkpoint_path, 'wb') as f:\n",
    "  pickle.dump(rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_of_pilot = {pilot_id: list(zip(*rollouts))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, '%s_pilot_eval.pkl' % pilot_id), 'wb') as f:\n",
    "  pickle.dump(eval_of_pilot, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate with copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copilot_policy(obs):\n",
    "  with tf.variable_scope(copilot_scope, reuse=None):\n",
    "    pilot_action = human_pilot_policy(obs[None, :n_obs_dim])\n",
    "    pilot_tol = 1 if pilot_action in [5, 6, 7] else 0\n",
    "    return raw_copilot_policy._act(\n",
    "      obs, \n",
    "      pilot_tol=pilot_tol, \n",
    "      pilot_action=pilot_action\n",
    "    )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_eval_eps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_env = make_co_env(pilot_policy=copilot_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_env.render()\n",
    "co_env.unwrapped.viewer.window.on_key_press = key_press\n",
    "co_env.unwrapped.viewer.window.on_key_release = key_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_rollouts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assisted_rollouts_checkpoint_path = os.path.join(data_dir, '%s_assisted_rollouts_checkpoint.pkl' % pilot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(assisted_rollouts_checkpoint_path, 'rb') as f:\n",
    "  assisted_rollouts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(assisted_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(n_eval_eps - len(assisted_rollouts)):\n",
    "  print('This will be episode %d of %d' % (len(assisted_rollouts)+1, n_eval_eps))\n",
    "  assisted_rollouts.append(run_ep(copilot_policy, co_env, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(assisted_rollouts_checkpoint_path, 'wb') as f:\n",
    "  pickle.dump(assisted_rollouts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_of_copilot = {pilot_id: list(zip(*assisted_rollouts))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, '%s_copilot_eval.pkl' % pilot_id), 'wb') as f:\n",
    "  pickle.dump(eval_of_copilot, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
